{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Build a ZRP Model Using Your Own Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to illustrate how to use ZRP_Build, a class that generates a new, custom ZRP model trained off of user input data. You must supply standard ZRP requirements including name and address, in addition to race to build the custom model-pipeline. The pipeline, model, and supporting data are saved automatically to \"./artifacts/experiments/{zrp_model_name}/\" in the support files path defined.\n",
    "\n",
    "This notebook is not intended to display ZRP performance. The dataset used is incredibly small for the purpose of displaying quickly how the model can be trained. To view a notebook displaying ZRP performance, see https://github.com/zestai/zrp/blob/main/examples/ZRP-Tutorial.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config Completer.use_jedi=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join, expanduser, dirname\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(action='ignore')\n",
    "home = expanduser('~')\n",
    "\n",
    "src_path = os.getcwd()\n",
    "root = os.path.join(src_path, \"../..\")\n",
    "sys.path.append(src_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zrp import ZRP\n",
    "from zrp.modeling import ZRP_Build\n",
    "from zrp.prepare.utils import load_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cleanup the directory structure \n",
    "if os.path.exists('artifacts'):\n",
    "    shutil.rmtree('artifacts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load sample data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 14)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sample = load_file(root + \"/tests/data/sm_1.csv\")\n",
    "input_sample['ZEST_KEY'] = input_sample.index.astype(str)  #must specify key to establish correspondence between inputs and outputs\n",
    "\n",
    "input_sample.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the input sample columns. As detailed in ZRP Build docstrings, the following columns are needed: first_name, middle_name, last_name, house_number, street_address, city, state, zip_code, race."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Invoke ZRP Build on the sample data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ZRP Build provides functionality for you to specify where to put artifacts folder & its files (pipeline, model(s), and supporting data), generated during intermediate steps. This is the parameter, 'file_path' If this is not specified, the artifacts folder is dumped in the same folder where the function is called from. \n",
    "\n",
    "The ZRP consists of a waterfall of 3 models: block group, census tract, and zip code. For a refresher on this architecture, please see https://github.com/zestai/zrp/blob/main/model_report.rst#prediction-process. When building your own ZRP model using ZRP Build, these three models are trained. To reflect the structure of the ZRP module, ZRP Build places the 3 generated models and associated files in distinct folders named for the geo level. These three folders are stored in the following directory: '[file_path]/artifacts/experiments/[zrp_model_name/]', where 'file_path' is the user defined or default path to the 'artifacts' parent directory. \n",
    "\n",
    "'zrp_model_name' is another relevant parameter into ZRP Build that specifies the name of this new model you are building. Ultimately, uniquely defining 'file_path' and 'zrp_model_name' will avoid overwriting previously built models in subsequent runs of ZRP Build."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "  0%|          | 0/6 [00:00<?, ?it/s]\u001b[A[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "100%|██████████| 6/6 [00:00<00:00, 8659.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is loaded\n",
      "   [Start] Validating input data\n",
      "     Number of observations: 6\n",
      "     Is key unique: True\n",
      "       (Warning!!) middle_name is 16.666666666666664% missing\n",
      "   [Completed] Validating input data\n",
      "\n",
      "   Formatting P1\n",
      "   Formatting P2\n",
      "   reduce whitespace\n",
      "\n",
      "[Start] Preparing geo data\n",
      "\n",
      "  The following states are included in the data: ['SC']\n",
      "   ... on state: SC\n",
      "\n",
      "   Data is loaded\n",
      "   [Start] Processing geo data\n",
      "      ...address cleaning\n",
      "      ...replicating address\n",
      "         ...Base\n",
      "         ...Number processing...\n",
      "         House number dataframe expansion is complete! (n=6)\n",
      "         ...Base\n",
      "         ...Map street suffixes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of   6 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         ...Mapped & split by street suffixes...\n",
      "         ...Number processing...\n",
      "\n",
      "         Address dataframe expansion is complete! (n=7)\n",
      "      ...formatting\n",
      "   [Completed] Processing geo data\n",
      "   [Start] Mapping geo data\n",
      "      ...merge user input & lookup table\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  3.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ...mapping\n",
      "   [Completed] Validating input geo data\n",
      "Directory already exists\n",
      "...Output saved\n",
      "   [Completed] Mapping geo data\n",
      "\n",
      "[Completed] Preparing geo data\n",
      "\n",
      "[Start] Preparing ACS data\n",
      "   [Start] Validating ACS input data\n",
      "     Number of observations: 6\n",
      "     Is key unique: True\n",
      "       (Warning!!) middle_name is 16.666666666666664% missing\n",
      "\n",
      "   [Completed] Validating ACS input data\n",
      "\n",
      "   ...loading ACS lookup tables\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ... combining ACS & user input data\n",
      " ...Copy dataframes\n",
      " ...Block group\n",
      " ...Census tract\n",
      " ...Zip code\n",
      " ...No match\n",
      " ...Merge\n",
      " ...Merging complete\n",
      "[Complete] Preparing ACS data\n",
      "\n",
      "=========================\n",
      "BUILDING block_group MODEL.\n",
      "=========================\n",
      "\n",
      "Dropping ['zip_code', 'census_tract'] features\n",
      "    ...Len features to keep list:  98\n",
      "    ...Data shape pre feature drop:  (16, 203)\n",
      "    ...Data shape post feature drop:  (16, 97)\n",
      "Directory already exists\n",
      "...Output saved\n",
      "...Output saved\n",
      "...Output saved\n",
      "...Output saved\n",
      "Post-sampling shape:  (6, 14)\n",
      "\n",
      "\n",
      "Unique train labels:  ['BLACK' 'WHITE' 'AIAN']\n",
      "Unique test labels:  ['AIAN' 'WHITE']\n",
      "\n",
      "---\n",
      "Saving raw data\n",
      "...Output saved\n",
      "...Output saved\n",
      "\n",
      "---\n",
      "Building pipeline\n",
      "\n",
      "---\n",
      "Fitting pipeline\n",
      "[Pipeline] ..... (step 1 of 8) Processing Drop Features, total=   0.0s\n",
      "Pass through\n",
      "[Pipeline] .. (step 2 of 8) Processing Compound Name FE, total=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s][Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "100%|██████████| 1/1 [00:00<00:00, 1328.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............ (step 3 of 8) Processing App FE, total=   0.1s\n",
      "[Pipeline] ............ (step 4 of 8) Processing ACS FE, total=   0.1s\n",
      "[Pipeline] .. (step 5 of 8) Processing Name Aggregation, total=   0.1s\n",
      "[Pipeline] . (step 6 of 8) Processing Drop Features (2), total=   0.0s\n",
      "[Pipeline] ............ (step 7 of 8) Processing Impute, total=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "  0%|          | 0/1 [00:00<?, ?it/s][Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "100%|██████████| 1/1 [00:00<00:00, 1528.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline]  (step 8 of 8) Processing Correlated Feature Selection, total=   0.1s\n",
      "Directory already exists\n",
      "\n",
      "---\n",
      "Transforming FE data\n",
      "Pass through\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---\n",
      "Saving FE data\n",
      "...Output saved\n",
      "\n",
      "---\n",
      "building zrp_model\n",
      "\n",
      "---\n",
      "fitting zrp_model\n",
      "Directory already exists\n",
      "Completed building block_group model.\n",
      "=========================\n",
      "BUILDING census_tract MODEL.\n",
      "=========================\n",
      "\n",
      "Dropping ['block_group', 'zip_code'] features\n",
      "    ...Len features to keep list:  148\n",
      "    ...Data shape pre feature drop:  (16, 203)\n",
      "    ...Data shape post feature drop:  (16, 147)\n",
      "Directory already exists\n",
      "...Output saved\n",
      "...Output saved\n",
      "...Output saved\n",
      "...Output saved\n",
      "Post-sampling shape:  (6, 14)\n",
      "\n",
      "\n",
      "Unique train labels:  ['BLACK' 'WHITE' 'AIAN']\n",
      "Unique test labels:  ['AIAN' 'WHITE']\n",
      "\n",
      "---\n",
      "Saving raw data\n",
      "...Output saved\n",
      "...Output saved\n",
      "\n",
      "---\n",
      "Building pipeline\n",
      "\n",
      "---\n",
      "Fitting pipeline\n",
      "[Pipeline] ..... (step 1 of 8) Processing Drop Features, total=   0.0s\n",
      "Pass through\n",
      "[Pipeline] .. (step 2 of 8) Processing Compound Name FE, total=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s][Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "100%|██████████| 1/1 [00:00<00:00, 1216.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............ (step 3 of 8) Processing App FE, total=   0.1s\n",
      "[Pipeline] ............ (step 4 of 8) Processing ACS FE, total=   0.1s\n",
      "[Pipeline] .. (step 5 of 8) Processing Name Aggregation, total=   0.1s\n",
      "[Pipeline] . (step 6 of 8) Processing Drop Features (2), total=   0.0s\n",
      "[Pipeline] ............ (step 7 of 8) Processing Impute, total=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "  0%|          | 0/1 [00:00<?, ?it/s][Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "100%|██████████| 1/1 [00:00<00:00, 1538.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline]  (step 8 of 8) Processing Correlated Feature Selection, total=   0.1s\n",
      "Directory already exists\n",
      "\n",
      "---\n",
      "Transforming FE data\n",
      "Pass through\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---\n",
      "Saving FE data\n",
      "...Output saved\n",
      "\n",
      "---\n",
      "building zrp_model\n",
      "\n",
      "---\n",
      "fitting zrp_model\n",
      "Directory already exists\n",
      "Completed building census_tract model.\n",
      "=========================\n",
      "BUILDING zip_code MODEL.\n",
      "=========================\n",
      "\n",
      "Dropping ['block_group', 'census_tract'] features\n",
      "    ...Len features to keep list:  165\n",
      "    ...Data shape pre feature drop:  (20, 203)\n",
      "    ...Data shape post feature drop:  (20, 164)\n",
      "Directory already exists\n",
      "...Output saved\n",
      "...Output saved\n",
      "...Output saved\n",
      "...Output saved\n",
      "Post-sampling shape:  (6, 14)\n",
      "\n",
      "\n",
      "Unique train labels:  ['WHITE' 'AAPI' 'BLACK' 'AIAN']\n",
      "Unique test labels:  ['AIAN' 'BLACK' 'WHITE']\n",
      "\n",
      "---\n",
      "Saving raw data\n",
      "...Output saved\n",
      "...Output saved\n",
      "\n",
      "---\n",
      "Building pipeline\n",
      "\n",
      "---\n",
      "Fitting pipeline\n",
      "[Pipeline] ..... (step 1 of 8) Processing Drop Features, total=   0.0s\n",
      "Pass through\n",
      "[Pipeline] .. (step 2 of 8) Processing Compound Name FE, total=   0.0s\n",
      "[Pipeline] ............ (step 3 of 8) Processing App FE, total=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s][Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "100%|██████████| 1/1 [00:00<00:00, 1210.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............ (step 4 of 8) Processing ACS FE, total=   0.1s\n",
      "[Pipeline] .. (step 5 of 8) Processing Name Aggregation, total=   0.1s\n",
      "[Pipeline] . (step 6 of 8) Processing Drop Features (2), total=   0.0s\n",
      "[Pipeline] ............ (step 7 of 8) Processing Impute, total=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "  0%|          | 0/1 [00:00<?, ?it/s][Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "100%|██████████| 1/1 [00:00<00:00, 1499.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline]  (step 8 of 8) Processing Correlated Feature Selection, total=   0.0s\n",
      "Directory already exists\n",
      "\n",
      "---\n",
      "Transforming FE data\n",
      "Pass through\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---\n",
      "Saving FE data\n",
      "...Output saved\n",
      "\n",
      "---\n",
      "building zrp_model\n",
      "\n",
      "---\n",
      "fitting zrp_model\n",
      "Directory already exists\n",
      "Completed building zip_code model.\n",
      "\n",
      "##############################\n",
      "Custom ZRP model build complete.\n",
      "CPU times: user 13min 20s, sys: 2.9 s, total: 13min 23s\n",
      "Wall time: 1min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "zest_race_predictor = ZRP_Build(zrp_model_name='test_model_small_data') \n",
    "zest_race_predictor.fit()\n",
    "output = zest_race_predictor.transform(input_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proxy using newly build ZRP model\n",
    "Make sure to specify the path to where the model is outputted (the parameter, 'pipe_path'). This includes appending 'artifacts/'experiments'/[zrp_model_name]' to your specified 'file_path'. In this case, we did not specify a 'file_path', thus the default is the path where this code is being run from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 14)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sample = load_file(root + \"/tests/data/sm_3.csv\")\n",
    "test_sample['ZEST_KEY'] = test_sample.index.astype(str)  #must specify key to establish correspondence between inputs and outputs\n",
    "\n",
    "test_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "100%|██████████| 5/5 [00:00<00:00, 6824.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################################\n",
      "Processing rows: 0:25000\n",
      "####################################\n",
      "Data is loaded\n",
      "   [Start] Validating input data\n",
      "     Number of observations: 5\n",
      "     Is key unique: True\n",
      "       (Warning!!) middle_name is 20.0% missing\n",
      "Directory already exists\n",
      "   [Completed] Validating input data\n",
      "\n",
      "   Formatting P1\n",
      "   Formatting P2\n",
      "   reduce whitespace\n",
      "\n",
      "[Start] Preparing geo data\n",
      "\n",
      "  The following states are included in the data: ['SC']\n",
      "   ... on state: SC\n",
      "\n",
      "   Data is loaded\n",
      "   [Start] Processing geo data\n",
      "      ...address cleaning\n",
      "      ...replicating address\n",
      "         ...Base\n",
      "         ...Number processing...\n",
      "         House number dataframe expansion is complete! (n=5)\n",
      "         ...Base\n",
      "         ...Map street suffixes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         ...Mapped & split by street suffixes...\n",
      "         ...Number processing...\n",
      "\n",
      "         Address dataframe expansion is complete! (n=5)\n",
      "      ...formatting\n",
      "   [Completed] Processing geo data\n",
      "   [Start] Mapping geo data\n",
      "      ...merge user input & lookup table\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ...mapping\n",
      "   [Completed] Validating input geo data\n",
      "Directory already exists\n",
      "...Output saved\n",
      "   [Completed] Mapping geo data\n",
      "\n",
      "[Completed] Preparing geo data\n",
      "\n",
      "[Start] Preparing ACS data\n",
      "   [Start] Validating ACS input data\n",
      "     Number of observations: 5\n",
      "     Is key unique: True\n",
      "       (Warning!!) middle_name is 20.0% missing\n",
      "\n",
      "   [Completed] Validating ACS input data\n",
      "\n",
      "   ...loading ACS lookup tables\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ... combining ACS & user input data\n",
      " ...Copy dataframes\n",
      " ...Block group\n",
      " ...Census tract\n",
      " ...Zip code\n",
      " ...No match\n",
      " ...Merge\n",
      " ...Merging complete\n",
      "[Complete] Preparing ACS data\n",
      "\n",
      "   [Start] Validating pipeline input data\n",
      "     Number of observations: 16\n",
      "     Is key unique: False\n",
      "       (Warning!!) middle_name is 12.5% missing\n",
      "   [Completed] Validating pipeline input data\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s][Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "100%|██████████| 1/1 [00:00<00:00, 1346.49it/s]\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "  0%|          | 0/1 [00:00<?, ?it/s][Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "100%|██████████| 1/1 [00:00<00:00, 1588.15it/s]\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ...Proxies generated\n",
      "...Output saved\n",
      "...Output saved\n",
      "CPU times: user 26.5 s, sys: 2.53 s, total: 29 s\n",
      "Wall time: 26.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "zest_race_predictor = ZRP(file_path = 'artifacts/experiments/test_model_small_data',\n",
    "                          pipe_path='artifacts/experiments/test_model_small_data',\n",
    "                         runname='custom_outputs')\n",
    "zest_race_predictor.fit()\n",
    "zrp_output = zest_race_predictor.transform(test_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "100%|██████████| 5/5 [00:00<00:00, 7679.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory already exists\n",
      "####################################\n",
      "Processing rows: 0:25000\n",
      "####################################\n",
      "Data is loaded\n",
      "   [Start] Validating input data\n",
      "     Number of observations: 5\n",
      "     Is key unique: True\n",
      "       (Warning!!) middle_name is 20.0% missing\n",
      "Directory already exists\n",
      "   [Completed] Validating input data\n",
      "\n",
      "   Formatting P1\n",
      "   Formatting P2\n",
      "   reduce whitespace\n",
      "\n",
      "[Start] Preparing geo data\n",
      "\n",
      "  The following states are included in the data: ['SC']\n",
      "   ... on state: SC\n",
      "\n",
      "   Data is loaded\n",
      "   [Start] Processing geo data\n",
      "      ...address cleaning\n",
      "      ...replicating address\n",
      "         ...Base\n",
      "         ...Number processing...\n",
      "         House number dataframe expansion is complete! (n=5)\n",
      "         ...Base\n",
      "         ...Map street suffixes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         ...Mapped & split by street suffixes...\n",
      "         ...Number processing...\n",
      "\n",
      "         Address dataframe expansion is complete! (n=5)\n",
      "      ...formatting\n",
      "   [Completed] Processing geo data\n",
      "   [Start] Mapping geo data\n",
      "      ...merge user input & lookup table\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ...mapping\n",
      "   [Completed] Validating input geo data\n",
      "Directory already exists\n",
      "...Output saved\n",
      "   [Completed] Mapping geo data\n",
      "\n",
      "[Completed] Preparing geo data\n",
      "\n",
      "[Start] Preparing ACS data\n",
      "   [Start] Validating ACS input data\n",
      "     Number of observations: 5\n",
      "     Is key unique: True\n",
      "       (Warning!!) middle_name is 20.0% missing\n",
      "\n",
      "   [Completed] Validating ACS input data\n",
      "\n",
      "   ...loading ACS lookup tables\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ... combining ACS & user input data\n",
      " ...Copy dataframes\n",
      " ...Block group\n",
      " ...Census tract\n",
      " ...Zip code\n",
      " ...No match\n",
      " ...Merge\n",
      " ...Merging complete\n",
      "[Complete] Preparing ACS data\n",
      "\n",
      "   [Start] Validating pipeline input data\n",
      "     Number of observations: 16\n",
      "     Is key unique: False\n",
      "       (Warning!!) middle_name is 12.5% missing\n",
      "   [Completed] Validating pipeline input data\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s][Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "100%|██████████| 1/1 [00:00<00:00, 1222.12it/s]\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "  0%|          | 0/1 [00:00<?, ?it/s][Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 15 concurrent workers.\n",
      "100%|██████████| 1/1 [00:00<00:00, 1465.00it/s]\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ...Proxies generated\n",
      "...Output saved\n",
      "...Output saved\n",
      "CPU times: user 26.6 s, sys: 2.46 s, total: 29.1 s\n",
      "Wall time: 26.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "zest_race_predictor = ZRP(file_path = 'artifacts/experiments/test_model_small_data', pipe_path='artifacts/experiments/test_model_small_data')\n",
    "zest_race_predictor.fit()\n",
    "zrp_output = zest_race_predictor.transform(test_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note*: The following output data frame shouldn't be evaluated for performance/accuracy. As stated above, this notebook trains on an insignificant amount of training data for the purpose of demonstrating quickly how to use ZRP Build. A larger dataset is necessary to build a model with strong performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ZRP v0.3.3",
   "language": "python",
   "name": "zrp_v0.3.3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
